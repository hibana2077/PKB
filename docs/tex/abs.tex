\begin{abstract}
Ultra-Fine-Grained Visual Categorization (UFGVC) often avoids image-level augmentations because random edits may obliterate discriminative cues. We revisit this assumption and introduce \textbf{PatchKeep-Blur (PKB)}, a zero-parameter, plug-and-play augmentation that applies patch-wise Gaussian low-pass filtering to a controllable subset of an image while guaranteeing a keep-sharp area. PKB offers three placement policies (random, dispersed, contiguous) that respectively encourage global integration, occlusion/defocus robustness, and a baseline mixing of contexts. We provide a frequency-space interpretation showing that PKB acts as a spatially selective high-frequency suppression that mitigates texture shortcuts and induces shape bias, and derive a combinatorial upper bound on the probability of fully blurring a discriminative region. Without extra networks, losses, or meta-learning, PKB consistently improves strong FGVC baselines across UFGVC benchmarks: for example, on \textit{Soybean} it lifts accuracy from 56.17\% (Mix-ViT) and 47.17\% (CLE-ViT) to 65.70\% (+9.53 and +18.53 percentage points), on \textit{Cotton80} to 70.00\% (+6.67 over CLE-ViT), with similar gains on \textit{SoyGene} (+3.86) and \textit{SoyAgeing} (+2.04). PKB is simple, label-safe, and orthogonal to feature-space augmentation; we expect it to serve as a broadly useful default for fine-grained recognition and robustness.
\end{abstract}